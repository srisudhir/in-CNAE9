{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us use a sample string to create our preprocessing functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '''\n",
    "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we can even verify what files are there in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.anaconda', '.conda', '.condarc', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.matplotlib', '20171021_Batch31_CSE7321c_Lab01_ANN.pdf', '20171022_Batch31_CSE7321c_Lab02_DL.pdf', 'Anaconda3', 'AppData', 'Application Data', 'BostonHousing.csv', 'cifar_pca_test.npz', 'cifar_pca_train.npz', 'class 1.ipynb', 'Contacts', 'Cookies', 'creditcard.csv', 'CustomerData.csv', 'DAE_FGen_cifar10_pca.ipynb', 'DAE_l1_model.h5', 'Desktop', 'Documents', 'Downloads', 'EconomistData.csv', 'Favorites', 'fraud_detection.h5', 'fraud_detection.ipynb', 'IntelGraphicsProfiles', 'landdata.csv', 'Links', 'Local Settings', 'minst1.h5', 'mnist.h5', 'Music', 'My Documents', 'negative.txt', 'NetHood', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{d1f6397f-715c-11e6-9628-b3bcdeface57}.TM.blf', 'NTUSER.DAT{d1f6397f-715c-11e6-9628-b3bcdeface57}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{d1f6397f-715c-11e6-9628-b3bcdeface57}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'Pictures', 'positive.txt', 'PrintHood', 'Recent', 'Saved Games', 'SDAE_cifar10_pca-Copy1.ipynb', 'SDAE_cifar10_pca.ipynb', 'SDAE_l1_model.h5', 'SDAE_l2_model.h5', 'Searches', 'SendTo', 'Sri Sudhir Viswanathan.ipynb', 'Sri_Sudhir_sentimentAnalysis.ipynb', 'StackedDAE.zip', 'Start Menu', 'Templates', 'test_solved.csv', 'test_solved_2.csv', 'text_mine.txt', 'text_mining_content.docx', 'Text_PreProcessing_assignment.ipynb', 'Tracing', 'train_solved.csv', 'u.data', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled10.ipynb', 'Untitled11.ipynb', 'Untitled12.ipynb', 'Untitled13.ipynb', 'Untitled2.ipynb', 'Untitled3.ipynb', 'Untitled4.ipynb', 'Untitled5.ipynb', 'Untitled6.ipynb', 'Untitled7.ipynb', 'Untitled8.ipynb', 'Untitled9.ipynb', 'Videos', 'XOR Demo in Keras.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Once we have our required text file(s)/folder(s) in the working directory we use the following function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text_mine.txt\", \"r\")as text :\n",
    "    string = text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'do', \"n't\", 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "tokens = nltk.tokenize.word_tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\", 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.', 'The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.', 'To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.', 'My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.', 'Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.']\n",
      "\n",
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "\n",
      "\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
      "\n",
      "\n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
      "\n",
      "\n",
      "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
      "\n",
      "\n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
      "\n",
      "\n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "sentences = nltk.tokenize.sent_tokenize(string)\n",
    "print(sentences)\n",
    "for sent in sentences:\n",
    "    print('\\n')\n",
    "    print(sent),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#case lowering\n",
    "#lower_tokens = []\n",
    "#for token in tokens:\n",
    "#    lower_tokens.append(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', '.', 'perfect']\n"
     ]
    }
   ],
   "source": [
    "tokens= [token for token in lower_tokens if token not in stop]\n",
    "print(tokens[:20])#this shows the 12 words which satisfy the above condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', \"n't\", 'train', 'leatherhead', ',', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', '.', 'perfect']\n"
     ]
    }
   ],
   "source": [
    "#lemetizing\n",
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(tokens[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox. Jumped over the lazy dog.\n",
      "the quick brown fox. jumped over the lazy dog.\n",
      "['the quick brown fox.', 'jumped over the lazy dog.']\n",
      "[['the', 'quick', 'brown', 'fox', '.'], ['jumped', 'over', 'the', 'lazy', 'dog', '.']]\n",
      "['quick', 'brown', 'fox.', 'jumped', 'over', 'lazy', 'dog.']\n",
      "['quick', 'brown', 'fox.', 'jumped', 'over', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "source": [
    "input_str1 = 'The quick brown fox. Jumped over the lazy dog.'\n",
    "s = []\n",
    "def process_text(input_str):\n",
    "    print(input_str)\n",
    "    #1. lowercase convertion\n",
    "    lower = input_str.lower()\n",
    "    print(lower)\n",
    "    #2. sentence tokenization\n",
    "    s_sents = nltk.sent_tokenize(lower)\n",
    "    print(s_sents)\n",
    "    #word tokenize on each sentence\n",
    "    w_tokenize = [nltk.word_tokenize(token) for token in s_sents] \n",
    "    print(w_tokenize)\n",
    "    #4.stopwords removal\n",
    "    stop_words = ['the']\n",
    "    stop_words_removed = [word for word in lower.split() if word not in stop_words]\n",
    "    print(stop_words_removed)\n",
    "    #4lemmetizing\n",
    "    lematized_words = [lmtzr.lemmatize(token) for token in stop_words_removed]\n",
    "    print(lematized_words)\n",
    "#calling function\n",
    "process_text('The quick brown fox. Jumped over the lazy dog.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
