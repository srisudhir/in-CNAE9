{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b7cbf946ed6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text.txt'"
     ]
    }
   ],
   "source": [
    "with open('text.txt', 'r') as file_:\n",
    "    string = file_.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c1ae24c37809>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwc\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "wc =wordcloud.WordCloud()\n",
    "img = wc.generate_from_text(string)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_freq = nltk.FreqDist()\n",
    "unigram_freq = nltk.FreqDist()\n",
    "corpus_tokens = nltk.tokenize.word_tokenize(string)\n",
    "unigrams = list(nltk.ngrams(corpus_tokens, 1))\n",
    "bigrams = list(nltk.ngrams(corpus_tokens,2))\n",
    "for unigram_token in unigrams:\n",
    "    unigram_freq[unigram_token] +=1\n",
    "print('the most commeon 10 unigrams are:',unigram_freq.most_common(10))\n",
    "for bigram_token in bigrams:\n",
    "    bigram_freq[bigram_token] += 1\n",
    "print('the most commeon 10 bigrams are:',bigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before PreProcessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length is: 2237\n",
      "Most Frequent Words are: [(' ', 378), ('e', 166), ('i', 157), ('t', 132), ('n', 130), ('a', 120), ('s', 109), ('o', 99), ('r', 94), ('l', 68)]\n",
      "Frequency of words: 2237\n",
      "The word length : 64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c0945a851e47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The word length :'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0munique_words\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "l=len(string)\n",
    "print('The Length is:',l)\n",
    "freq_dist = nltk.FreqDist(string)\n",
    "F = freq_dist.most_common(10)\n",
    "print('Most Frequent Words are:',F)\n",
    "word_features= set(string)\n",
    "freq=0\n",
    "for word in string:\n",
    "    freq=freq+1\n",
    "print('Frequency of words:',freq)\n",
    "wl=len(word_features)\n",
    "print('The word length :',wl)\n",
    "count=0\n",
    "unique_words= set(tokens)\n",
    "for word in unique_words:\n",
    "    count = count+1\n",
    "print (\"unique words =\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ad6d1baa46de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.sent_tokenize(string)\n",
    "num=0\n",
    "for sent in sentences:\n",
    "    num=num+1\n",
    "    print('\\n')\n",
    "    print(sent)\n",
    "print(\"no.of sentences in the text are:\",num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '.', 'Title', ':', 'CNAE-9', '2', '.', 'Source', 'Information', '-', 'Data', 'set', 'was', 'initially', 'used', 'in', ':', 'Patrick', 'Marques', 'Ciarelli', ',', 'Elias', 'Oliveira', ',', '``', 'Agglomeration', 'and', 'Elimination', 'of', 'Terms', 'for', 'Dimensionality', 'Reduction', \"''\", ',', 'Ninth', 'International', 'Conference', 'on', 'Intelligent', 'Systems', 'Design', 'and', 'Applications', ',', 'pp', '.', '547-552', ',', '2009', '3', '.', 'Past', 'Usage', ':', '-', 'Patrick', 'Marques', 'Ciarelli', ',', 'Elias', 'Oliveira', ',', '``', 'Agglomeration', 'and', 'Elimination', 'of', 'Terms', 'for', 'Dimensionality', 'Reduction', \"''\", ',', 'Ninth', 'International', 'Conference', 'on', 'Intelligent', 'Systems', 'Design', 'and', 'Applications', ',', 'pp', '.', '547-552', ',', '2009', ':', '-', 'Feature', 'selection', '(', '900', 'instances', 'for', 'training', 'and', '180', 'instances', 'for', 'test', ')', ':', '-', 'Best', 'results', 'using', 'kNN', '(', 'k=1', ')', ':', '50', 'dimensions', ':', '87.78', '%', '(', 'LSI', ')', '100', 'dimensions', ':', '92.78', '%', '(', 'LSI', ')', '150', 'dimensions', ':', '92.22', '%', '(', 'LSI', ')', '200', 'dimensions', ':', '92.78', '%', '(', 'MI', ')', '250', 'dimensions', ':', '92.78', '%', '(', 'MI', ')', '-', 'Patrick', 'Marques', 'Ciarelli', ',', 'Elias', 'Oliveira', ',', 'Evandro', 'O.', 'T.', 'Salles', ',', '``', 'An', 'Evolving', 'System', 'Based', 'on', 'Probabilistic', 'Neural', 'Network', \"''\", ',', 'Brazilian', 'Symposium', 'on', 'Artificial', 'Neural', 'Network', ',', '2010', ':', '-', 'Incremental', 'learning', '(', 'no', 'off-line', 'training', ')', ':', '-', 'Best', 'result', ':', '88.71', '%', '(', 'ePNN', ')', '4', '.', 'Relevant', 'Information', ':', '-', 'This', 'is', 'a', 'data', 'set', 'containing', '1080', 'documents', 'of', 'free', 'text', 'business', 'descriptions', 'of', 'Brazilian', 'companies', 'categorized', 'into', 'a', 'subset', 'of', '9', 'categories', 'cataloged', 'in', 'a', 'table', 'called', 'National', 'Classification', 'of', 'Economic', 'Activities', '(', 'Classifica', '?', '?', 'o', 'Nacional', 'de', 'Atividade', 'Econ', '?', 'micas', '-', 'CNAE', ')', '.', 'The', 'original', 'texts', 'were', 'pre-processed', 'to', 'obtain', 'the', 'current', 'data', 'set', ':', 'initially', ',', 'it', 'was', 'kept', 'only', 'letters', 'and', 'then', 'it', 'was', 'removed', 'prepositions', 'of', 'the', 'texts', '.', 'Next', ',', 'the', 'words', 'were', 'transformed', 'to', 'their', 'canonical', 'form', '.', 'Finally', ',', 'each', 'document', 'was', 'represented', 'as', 'a', 'vector', ',', 'where', 'the', 'weight', 'of', 'each', 'word', 'is', 'its', 'frequency', 'in', 'the', 'document', '.', 'This', 'data', 'set', 'is', 'highly', 'sparse', '(', '99.22', '%', 'of', 'the', 'matrix', 'is', 'filled', 'with', 'zeros', ')', '.', '5', '.', 'Number', 'of', 'Instances', ':', '1080', '6', '.', 'Number', 'of', 'Attributes', ':', '857', '(', '1', 'category', ',', '856', 'word', 'frequency', ')', '7', '.', 'Attribute', 'Information', ':', '1.', 'category', ':', 'range', '1', '-', '9', '(', 'integer', ')', '2', '-', '857.', 'word', 'frequency', ':', '(', 'integer', ')', '8', '.', 'Missing', 'Attribute', 'Values', ':', 'None', '9', '.', 'Class', 'Distribution', ':', 'the', 'categories', 'are', 'equally', 'distribuited', '.', '(', '120', 'instances', 'in', 'each', 'of', 'nine', 'categories', ')', 'Summary', 'Statistics', ':', 'Min', 'Max', 'Mean', 'SD', 'word', 'frequency', ':', '0', '4', '0.0082', '0.0948']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(string)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ba07b3586cab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^([0-9]+[a-zA-Z]+|\\d|+\\W)$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None, tokenizer=TreebankWordTokenizer().tokenize)\n",
    "tokenize = vectorizer.build_tokenizer()\n",
    "\n",
    "tokens = tokenize(string)\n",
    "tokens = [token for token in tokens if re.match('^([0-9]+[a-zA-Z]+|\\d|+\\W)$', token)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]\n",
    "print(lower_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "print(stop)\n",
    "tokens = [token for token in lower_tokens if token not in stop]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le =len(tokens)\n",
    "print('The length is :',le)\n",
    "freq_dist = nltk.FreqDist(tokens)\n",
    "F = freq_dist.most_common(10)\n",
    "print('Most Frequent Words are:',F)\n",
    "word_features= set(tokens)\n",
    "freq=0\n",
    "for word in tokens:\n",
    "    freq=freq+1\n",
    "print('The Frequency of word after PreProcessing:',freq)\n",
    "wl=len(word_features)\n",
    "print('The word length after PreProcessing :',wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "print(\"Extracting tf-idf features...\")\n",
    "#First we initiate an empty tfidf object with specific conditions\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))#max_df=0.95, min_df=2, stop_words='english' #USE HELP TO SEE WHAT EACH DOES)\n",
    "t0 = time()\n",
    "#Next we give the data for processing\n",
    "tfidf = tfidf_vectorizer.fit_transform(tokens)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense = tfidf.todense()\n",
    "dense.shape\n",
    "print(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(len(feature_names))\n",
    "feature_names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame(dense)\n",
    "x.columns = tfidf_vectorizer.get_feature_names()\n",
    "x['text'] = tokens\n",
    "x.to_csv('tex-tfidf.csv', index = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
